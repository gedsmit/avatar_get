{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Generating Faces","metadata":{}},{"cell_type":"markdown","source":"**The goal is to create new faces of anime characters using a Deep Convolutional Generative Adversarial Network (DCGAN).**\n\nHere is the same approach used on the Bored Ape Yacht Club dataset: https://www.kaggle.com/code/nassimyagoub/bored-ape-yacht-club-gan","metadata":{}},{"cell_type":"markdown","source":"## Importing libraries and loading data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport os \nimport matplotlib.pyplot as plt\nimport cv2\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport keras\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, LeakyReLU, Reshape, Flatten, Input\nfrom keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Conv2DTranspose\n\nfrom tensorflow.compat.v1.keras.layers import BatchNormalization","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-04T19:33:19.465191Z","iopub.execute_input":"2023-06-04T19:33:19.465816Z","iopub.status.idle":"2023-06-04T19:33:30.543063Z","shell.execute_reply.started":"2023-06-04T19:33:19.465771Z","shell.execute_reply":"2023-06-04T19:33:30.541453Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def list_images(basePath, contains=None):\n    # return the set of files that are valid\n    return list_files(basePath, validExts=(\".jpg\", \".jpeg\", \".png\", \".bmp\"), contains=contains)\n\ndef list_files(basePath, validExts=(\".jpg\", \".jpeg\", \".png\", \".bmp\"), contains=None):\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an image and should be processed\n            if ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename).replace(\" \", \"\\\\ \")\n                yield imagePath\n                \ndef load_images(directory='', size=(64,64)):\n    images = []\n    labels = []  # Integers corresponding to the categories in alphabetical order\n    label = 0\n    \n    imagePaths = list(list_images(directory))\n    \n    for path in imagePaths:\n        \n        if not('OSX' in path):\n        \n            path = path.replace('\\\\','/')\n\n            image = cv2.imread(path) #Reading the image with OpenCV\n            image = cv2.resize(image,size) #Resizing the image, in case some are not of the same size\n\n            images.append(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    \n    return images","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2023-06-04T19:33:30.545787Z","iopub.execute_input":"2023-06-04T19:33:30.546972Z","iopub.status.idle":"2023-06-04T19:33:30.561896Z","shell.execute_reply.started":"2023-06-04T19:33:30.546925Z","shell.execute_reply":"2023-06-04T19:33:30.560227Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"images=load_images('../input/data')","metadata":{"execution":{"iopub.status.busy":"2023-06-04T19:33:30.564905Z","iopub.execute_input":"2023-06-04T19:33:30.566091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Looking at some images","metadata":{}},{"cell_type":"code","source":"_,ax = plt.subplots(5,5, figsize = (8,8)) \nfor i in range(5):\n    for j in range(5):\n        ax[i,j].imshow(images[5*i+j])\n        ax[i,j].axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generative Adversarial Networks","metadata":{}},{"cell_type":"markdown","source":"The objective of a GAN is to train a data generator in order to imitate a given dataset.\nA GAN is similar to a zero sum game between two neural networks, the generator of data and a discriminator, trained to recognize original data from fakes created by the generator.","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://www.researchgate.net/profile/Jose-Benitez-Andrades/publication/339447623/figure/fig2/AS:862056077082627@1582541593714/A-vanilla-Convolutional-Neural-Network-CNN-representation.png\" width=\"800px\">","metadata":{}},{"cell_type":"markdown","source":"At each step, the discriminator is trained on a batch containing real and fake images. The generator is then trained to produce a batch of images.\n<br>**In order to create effective GANs on images, we must use convolutional layers in the discriminator and in the generator.**","metadata":{}},{"cell_type":"markdown","source":"In a Deep Convolutional GAN, the data generator has the following structure :","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://pytorch.org/tutorials/_images/dcgan_generator.png\" width=\"800px\">","metadata":{}},{"cell_type":"markdown","source":"It takes a noise vector as the input in order to diversify the potential outputs.\n<br><br>In a simplified way, after the training each dimension will correspond to a feature of the image, for example the hair shape of the character.","metadata":{}},{"cell_type":"markdown","source":"## Creating the GAN","metadata":{}},{"cell_type":"code","source":"class GAN():\n    def __init__(self):\n        self.img_shape = (64, 64, 3)\n        \n        self.noise_size = 100\n\n        optimizer = Adam(0.0002,0.5)\n\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss='binary_crossentropy', \n                                   optimizer=optimizer,\n                                   metrics=['accuracy'])\n\n        self.generator = self.build_generator()\n        self.generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n        \n        self.combined = Sequential()\n        self.combined.add(self.generator)\n        self.combined.add(self.discriminator)\n        \n        self.discriminator.trainable = False\n        \n        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n        \n        self.combined.summary()\n        \n    # Creating the generator, the large kernels in the convolutional layers allow the network to create complex structures.\n    def build_generator(self):\n        epsilon = 0.00001 # Small float added to variance to avoid dividing by zero in the BatchNorm layers.\n        noise_shape = (self.noise_size,)\n        \n        model = Sequential()\n        \n        model.add(Dense(4*4*512, activation='linear', input_shape=noise_shape))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Reshape((4, 4, 512)))\n        \n        model.add(Conv2DTranspose(512, kernel_size=[4,4], strides=[2,2], padding=\"same\",\n                                  kernel_initializer= keras.initializers.TruncatedNormal(stddev=0.02)))\n        model.add(BatchNormalization(momentum=0.9, epsilon=epsilon))\n        model.add(LeakyReLU(alpha=0.2))\n        \n        model.add(Conv2DTranspose(256, kernel_size=[4,4], strides=[2,2], padding=\"same\",\n                                  kernel_initializer= keras.initializers.TruncatedNormal(stddev=0.02)))\n        model.add(BatchNormalization(momentum=0.9, epsilon=epsilon))\n        model.add(LeakyReLU(alpha=0.2))\n        \n        model.add(Conv2DTranspose(128, kernel_size=[4,4], strides=[2,2], padding=\"same\",\n                                  kernel_initializer= keras.initializers.TruncatedNormal(stddev=0.02)))\n        model.add(BatchNormalization(momentum=0.9, epsilon=epsilon))\n        model.add(LeakyReLU(alpha=0.2))\n        \n        model.add(Conv2DTranspose(64, kernel_size=[4,4], strides=[2,2], padding=\"same\",\n                                  kernel_initializer= keras.initializers.TruncatedNormal(stddev=0.02)))\n        model.add(BatchNormalization(momentum=0.9, epsilon=epsilon))\n        model.add(LeakyReLU(alpha=0.2))\n        \n        model.add(Conv2DTranspose(3, kernel_size=[4,4], strides=[1,1], padding=\"same\",\n                                  kernel_initializer= keras.initializers.TruncatedNormal(stddev=0.02)))\n\n        # Standard activation for the generator of a GAN\n        model.add(Activation(\"tanh\"))\n        \n        model.summary()\n\n        noise = Input(shape=noise_shape)\n        img = model(noise)\n\n        return Model(noise, img)\n\n    def build_discriminator(self):\n\n        model = Sequential()\n\n        model.add(Conv2D(128, (3,3), padding='same', input_shape=self.img_shape))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization())\n        model.add(Conv2D(128, (3,3), padding='same'))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization())\n        model.add(MaxPooling2D(pool_size=(3,3)))\n        model.add(Dropout(0.2))\n\n        model.add(Conv2D(128, (3,3), padding='same'))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization())\n        model.add(Conv2D(128, (3,3), padding='same'))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization())\n        model.add(MaxPooling2D(pool_size=(3,3)))\n        model.add(Dropout(0.3))\n\n        model.add(Flatten())\n        model.add(Dense(128))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(128))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dense(1, activation='sigmoid'))\n        \n        model.summary()\n        \n        img = Input(shape=self.img_shape)\n        validity = model(img)\n\n        return Model(img, validity)\n\n    def train(self, epochs, batch_size=128, metrics_update=50, save_images=100, save_model=2000):\n\n        X_train = np.array(images)\n        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n\n        half_batch = int(batch_size / 2)\n        \n        mean_d_loss=[0,0]\n        mean_g_loss=0\n\n        for epoch in range(epochs):\n            idx = np.random.randint(0, X_train.shape[0], half_batch)\n            imgs = X_train[idx]\n\n            noise = np.random.normal(0, 1, (half_batch, self.noise_size))\n            gen_imgs = self.generator.predict(noise)\n\n            # Training the discriminator\n            \n            # The loss of the discriminator is the mean of the losses while training on authentic and fake images\n            d_loss = 0.5 * np.add(self.discriminator.train_on_batch(imgs, np.ones((half_batch, 1))),\n                                  self.discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1))))\n\n            # Training the generator\n            for _ in range(2):\n                noise = np.random.normal(0, 1, (batch_size, self.noise_size))\n\n                valid_y = np.array([1] * batch_size)\n                g_loss = self.combined.train_on_batch(noise, valid_y)\n            \n            mean_d_loss[0] += d_loss[0]\n            mean_d_loss[1] += d_loss[1]\n            mean_g_loss += g_loss\n            \n            # We print the losses and accuracy of the networks every 200 batches mainly to make sure the accuracy of the discriminator\n            # is not stable at around 50% or 100% (which would mean the discriminator performs not well enough or too well)\n            if epoch % metrics_update == 0:\n                print (\"%d [Discriminator loss: %f, acc.: %.2f%%] [Generator loss: %f]\" % (epoch, mean_d_loss[0]/metrics_update, 100*mean_d_loss[1]/metrics_update, mean_g_loss/metrics_update))\n                mean_d_loss=[0,0]\n                mean_g_loss=0\n            \n            # Saving 25 images\n            if epoch % save_images == 0:\n                self.save_images(epoch)\n            \n            # We save the architecture of the model, the weights and the state of the optimizer\n            # This way we can restart the training exactly where we stopped\n            if epoch % save_model == 0:\n                self.generator.save(\"generator_%d\" % epoch)\n                self.discriminator.save(\"discriminator_%d\" % epoch)\n\n    # Saving 25 generated images to have a representation of the spectrum of images created by the generator\n    def save_images(self, epoch):\n        noise = np.random.normal(0, 1, (25, self.noise_size))\n        gen_imgs = self.generator.predict(noise)\n        \n        # Rescale from [-1,1] into [0,1]\n        gen_imgs = 0.5 * gen_imgs + 0.5\n\n        fig, axs = plt.subplots(5,5, figsize = (8,8))\n\n        for i in range(5):\n            for j in range(5):\n                axs[i,j].imshow(gen_imgs[5*i+j])\n                axs[i,j].axis('off')\n\n        plt.show()\n        \n        fig.savefig(\"animeGenerated/Faces_%d.png\" % epoch)\n        plt.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the networks","metadata":{}},{"cell_type":"code","source":"#This folder will contain the images generated during the training\n!mkdir animeGenerated","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Things to keep in mind while training a GAN","metadata":{}},{"cell_type":"markdown","source":"**In the case of a GAN, cost functions may not converge using gradient descent.**\n<br/>Training a GAN is very different from training a regular neural network. The main difference come from the fact that the discriminator also evolves.\nTherefore, the cost function that the generator tries to minimize is also evolving.\n\n**The discriminator must be powerful enough to recognize small differences between the real and fake images.**\n<br/>If the discriminator does not perform well, the generator cannot progress.\nIf the accuracy of the discriminator stabilizes near 50%, the generator already tricks the discriminator well and does not need to improve.\n\n**The two networks must progress in synergy to avoid a diminished gradient during the training of the generator.**\n<br/>The discriminator must not be too effective at the beginning, if it is, the generator will not progress until it randomly creates a very convincing image.\n\nWe want to avoid **mode collapse** which is when the generator creates the same output no matter the input noise.\nIt leads to an overfitting from the discriminator which remembers the features of the fake, no matter how convincing it is.\n\n**A high learning rate creates a situation in which both networks overfit to exploit short term opponent weaknesses.**\n<br/>The **learning rate** of the optimizer must be carefully chosen.\nA high learning rate prevents the generator from reaching a certain level of details, it prevents the convergence.\nIt also leads to an overfitting from the discriminator which recognizes the most recent features made by the generator but forgets the previous ones.","metadata":{}},{"cell_type":"markdown","source":"### Training session","metadata":{}},{"cell_type":"markdown","source":"A high batch size leads to a more regular convergence.\n\nWe will save the model at the end of the training, we save the architecture of the model, the weights and the state of the optimizer. It allows us to restart the training exactly where we stopped.\n\nWe will look at a sample of images every 1000 epoch.","metadata":{}},{"cell_type":"code","source":"gan=GAN()\ngan.train(epochs=15001, batch_size=256, metrics_update=200, save_images=1000, save_model=15000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A few images are convincing and those images present different hair styles, hair colors or face orientations.\n\n**We avoided mode collapse** and the networks seem to have a good synergy with a stable accuracy at around 85% for the discriminator during most of the training.\n\nHowerver, the generator does not create globally convincing faces, for example some of them present different shape and colors between the two eyes.","metadata":{}},{"cell_type":"code","source":"!pip show tensorflow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip show keras","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}